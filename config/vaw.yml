model:
  fusion: txt2img
  # fusion: img2txt
  K: 1
  prompt_template: ["a photo of x x", "a photo of x", "a photo of x"]
  ctx_init: ["a photo of ", "a photo of ", "a photo of "]
  clip_model: "ViT-L/14"
  num_query_token: 1
  res_w: 0.8
  SA_K: 1
  width_img: 1024
  width_txt: 768
  ctx_dim: 768
  patch_size: 14
  image_size: 224
  en_dim: 1024
  depth: 4
  heads: 4
  mlp_dim: 2048
  de_dim: 1024
  embed_dim: 32
  n_embed: 200
  adapter_dim: 64
  adapter_dropout: 0.1
  mask_ratio: 0.75


train:
  dataset: vaw-czsl
  dataset_path: 
  lr1: 0.0005
  lr2: 4.5e-6
  lr_sgd: 0.0005
  attr_dropout: 0.3
  weight_decay: 0.00001
  context_length: 8
  train_batch_size: 2
  gradient_accumulation_steps: 2
  seed: 2134
  epochs: 50
  epoch_start: 0
  save_path: 
  best_model_metric: best_loss     #best_unseen  best_seen AUC best_loss best_hm
  save_model: True
  load_model: False     # False or model path
  att_obj_w: 0.01
  sp_w: 0.1
  vcd_weight: 1
  

test:
  eval_batch_size: 32
  open_world: False
  load_model: 
  text_encoder_batch_size: 36
  threshold: 0.4
  threshold_trials: 50
  bias: 0.001